{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "august-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU,Dense\n",
    "import numpy as np\n",
    "# from nltk.translate import meteor_score\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "breathing-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "latent_dim = 256\n",
    "num_samples = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "piano-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "output_texts = []\n",
    "input_characters = set()\n",
    "output_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, output_text, _ = line.split('\\t')\n",
    "    \n",
    "    # We use 'tab' as the 'start sequence' character\n",
    "    # for the targets, and '\\n' as the 'end sequence' character.\n",
    "    \n",
    "    output_text = '\\t' + output_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in output_text:\n",
    "        if char not in output_characters:\n",
    "            output_characters.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attractive-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "earlier-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "output_characters = sorted(list(output_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(output_characters)\n",
    "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in output_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greater-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 20000\n",
      "Number of unique input Tokens: 74\n",
      "Number of unique output Tokens: 99\n",
      "Max sequence length for inputs: 17\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples:\", len(input_texts))\n",
    "print('Number of unique input Tokens:' , num_encoder_tokens)\n",
    "print('Number of unique output Tokens:' , num_decoder_tokens)\n",
    "print('Max sequence length for inputs:' , max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:' , max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hawaiian-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(input_characters)])\n",
    "\n",
    "output_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(output_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "comic-bunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({' ': 0,\n",
       "  '!': 1,\n",
       "  '\"': 2,\n",
       "  '$': 3,\n",
       "  '%': 4,\n",
       "  '&': 5,\n",
       "  \"'\": 6,\n",
       "  ',': 7,\n",
       "  '-': 8,\n",
       "  '.': 9,\n",
       "  '/': 10,\n",
       "  '0': 11,\n",
       "  '1': 12,\n",
       "  '2': 13,\n",
       "  '3': 14,\n",
       "  '4': 15,\n",
       "  '5': 16,\n",
       "  '6': 17,\n",
       "  '7': 18,\n",
       "  '8': 19,\n",
       "  '9': 20,\n",
       "  ':': 21,\n",
       "  '?': 22,\n",
       "  'A': 23,\n",
       "  'B': 24,\n",
       "  'C': 25,\n",
       "  'D': 26,\n",
       "  'E': 27,\n",
       "  'F': 28,\n",
       "  'G': 29,\n",
       "  'H': 30,\n",
       "  'I': 31,\n",
       "  'J': 32,\n",
       "  'K': 33,\n",
       "  'L': 34,\n",
       "  'M': 35,\n",
       "  'N': 36,\n",
       "  'O': 37,\n",
       "  'P': 38,\n",
       "  'Q': 39,\n",
       "  'R': 40,\n",
       "  'S': 41,\n",
       "  'T': 42,\n",
       "  'U': 43,\n",
       "  'V': 44,\n",
       "  'W': 45,\n",
       "  'Y': 46,\n",
       "  'a': 47,\n",
       "  'b': 48,\n",
       "  'c': 49,\n",
       "  'd': 50,\n",
       "  'e': 51,\n",
       "  'f': 52,\n",
       "  'g': 53,\n",
       "  'h': 54,\n",
       "  'i': 55,\n",
       "  'j': 56,\n",
       "  'k': 57,\n",
       "  'l': 58,\n",
       "  'm': 59,\n",
       "  'n': 60,\n",
       "  'o': 61,\n",
       "  'p': 62,\n",
       "  'q': 63,\n",
       "  'r': 64,\n",
       "  's': 65,\n",
       "  't': 66,\n",
       "  'u': 67,\n",
       "  'v': 68,\n",
       "  'w': 69,\n",
       "  'x': 70,\n",
       "  'y': 71,\n",
       "  'z': 72,\n",
       "  'é': 73},\n",
       " {'\\t': 0,\n",
       "  '\\n': 1,\n",
       "  ' ': 2,\n",
       "  '!': 3,\n",
       "  '$': 4,\n",
       "  '%': 5,\n",
       "  '&': 6,\n",
       "  \"'\": 7,\n",
       "  ',': 8,\n",
       "  '-': 9,\n",
       "  '.': 10,\n",
       "  '0': 11,\n",
       "  '1': 12,\n",
       "  '2': 13,\n",
       "  '3': 14,\n",
       "  '4': 15,\n",
       "  '5': 16,\n",
       "  '6': 17,\n",
       "  '7': 18,\n",
       "  '8': 19,\n",
       "  '9': 20,\n",
       "  ':': 21,\n",
       "  '?': 22,\n",
       "  'A': 23,\n",
       "  'B': 24,\n",
       "  'C': 25,\n",
       "  'D': 26,\n",
       "  'E': 27,\n",
       "  'F': 28,\n",
       "  'G': 29,\n",
       "  'H': 30,\n",
       "  'I': 31,\n",
       "  'J': 32,\n",
       "  'K': 33,\n",
       "  'L': 34,\n",
       "  'M': 35,\n",
       "  'N': 36,\n",
       "  'O': 37,\n",
       "  'P': 38,\n",
       "  'Q': 39,\n",
       "  'R': 40,\n",
       "  'S': 41,\n",
       "  'T': 42,\n",
       "  'U': 43,\n",
       "  'V': 44,\n",
       "  'W': 45,\n",
       "  'Y': 46,\n",
       "  'a': 47,\n",
       "  'b': 48,\n",
       "  'c': 49,\n",
       "  'd': 50,\n",
       "  'e': 51,\n",
       "  'f': 52,\n",
       "  'g': 53,\n",
       "  'h': 54,\n",
       "  'i': 55,\n",
       "  'j': 56,\n",
       "  'k': 57,\n",
       "  'l': 58,\n",
       "  'm': 59,\n",
       "  'n': 60,\n",
       "  'o': 61,\n",
       "  'p': 62,\n",
       "  'q': 63,\n",
       "  'r': 64,\n",
       "  's': 65,\n",
       "  't': 66,\n",
       "  'u': 67,\n",
       "  'v': 68,\n",
       "  'w': 69,\n",
       "  'x': 70,\n",
       "  'y': 71,\n",
       "  'z': 72,\n",
       "  '\\xa0': 73,\n",
       "  '«': 74,\n",
       "  '»': 75,\n",
       "  'À': 76,\n",
       "  'Ç': 77,\n",
       "  'É': 78,\n",
       "  'Ê': 79,\n",
       "  'Ô': 80,\n",
       "  'à': 81,\n",
       "  'â': 82,\n",
       "  'ç': 83,\n",
       "  'è': 84,\n",
       "  'é': 85,\n",
       "  'ê': 86,\n",
       "  'ë': 87,\n",
       "  'î': 88,\n",
       "  'ï': 89,\n",
       "  'ô': 90,\n",
       "  'ù': 91,\n",
       "  'û': 92,\n",
       "  'œ': 93,\n",
       "  '\\u2009': 94,\n",
       "  '‘': 95,\n",
       "  '’': 96,\n",
       "  '\\u202f': 97,\n",
       "  '‽': 98})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index , output_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deluxe-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype = 'float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length , num_decoder_tokens), dtype = 'float32')\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens) , dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sought-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,output_text) in enumerate(zip(input_texts,output_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1.\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1.\n",
    "    \n",
    "    for t, char in enumerate(output_text):\n",
    "        # decoder_output_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i,t,output_token_index[char]] = 1.\n",
    "        if t>0:\n",
    "            # decoder_output_data will be ahead by one timestep\n",
    "            # and will not include the start character\n",
    "            decoder_output_data[i,t-1,output_token_index[char]] = 1.\n",
    "    decoder_input_data[i,t+1:, output_token_index[' ']] = 1.\n",
    "    decoder_output_data[i , t: , output_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sophisticated-singapore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 74)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "backed-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "encoder_outputs, state = encoder(encoder_inputs)\n",
    "encoder_states = [state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "outdoor-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, state = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e8eed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "involved-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 33s 122ms/step - loss: 0.4857 - accuracy: 0.8550 - f1_score: 0.8752 - val_loss: 0.5797 - val_accuracy: 0.8263 - val_f1_score: 0.8530\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 29s 118ms/step - loss: 0.4730 - accuracy: 0.8586 - f1_score: 0.8780 - val_loss: 0.5717 - val_accuracy: 0.8299 - val_f1_score: 0.8568\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.4607 - accuracy: 0.8621 - f1_score: 0.8809 - val_loss: 0.5593 - val_accuracy: 0.8345 - val_f1_score: 0.8595\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 51s 204ms/step - loss: 0.4480 - accuracy: 0.8657 - f1_score: 0.8841 - val_loss: 0.5429 - val_accuracy: 0.8401 - val_f1_score: 0.8624\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 53s 213ms/step - loss: 0.4330 - accuracy: 0.8702 - f1_score: 0.8881 - val_loss: 0.5283 - val_accuracy: 0.8442 - val_f1_score: 0.8655\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.4218 - accuracy: 0.8732 - f1_score: 0.8907 - val_loss: 0.5179 - val_accuracy: 0.8463 - val_f1_score: 0.8674\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 53s 213ms/step - loss: 0.4112 - accuracy: 0.8765 - f1_score: 0.8931 - val_loss: 0.5093 - val_accuracy: 0.8485 - val_f1_score: 0.8696\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.4009 - accuracy: 0.8791 - f1_score: 0.8956 - val_loss: 0.4970 - val_accuracy: 0.8522 - val_f1_score: 0.8729\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 0.3905 - accuracy: 0.8823 - f1_score: 0.8980 - val_loss: 0.4866 - val_accuracy: 0.8552 - val_f1_score: 0.8760\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.3802 - accuracy: 0.8852 - f1_score: 0.9006 - val_loss: 0.4808 - val_accuracy: 0.8576 - val_f1_score: 0.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b9b381af10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# 'encoder_input_data' & 'decoder_input_data' into 'decoder_output_data'\n",
    "model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Run training:\n",
    "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy' , metrics = ['accuracy', f1_score])\n",
    "model.fit([encoder_input_data,decoder_input_data], decoder_output_data,\n",
    "         batch_size=batch_size,\n",
    "         epochs = epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Go.\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Quile est chande !\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Quile est chande !\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prends un conter.\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Restez pas !\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Restez pas !\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Restez pas !\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Restez pas !\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Restez pas !\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ProjectGurukul Project: English to French Translation \n",
      "Input sentence: Run.\n",
      "Decoded sentence: Restez pas !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define sampling models:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape = (latent_dim,))\n",
    "decoder_state_input_c = Input(shape = (latent_dim,))\n",
    "decoder_input_states = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "decoder_outputs,state_h,state_c = decoder_gru(decoder_inputs, initial_state=decoder_input_states)\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs]+decoder_input_states,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i,char) for char,i in input_token_index.items())\n",
    "reverse_output_char_index = dict(\n",
    "    (i,char) for char,i in output_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value= encoder_model.predict(input_seq)\n",
    "    output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    output_seq[0,0,output_token_index['\\t']] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentences = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict(\n",
    "            [output_seq]+ states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1, :])\n",
    "        sampled_char = reverse_output_char_index[sampled_token_index]\n",
    "        decoded_sentences += sampled_char\n",
    "\n",
    "        if(sampled_char == '\\n' or len(decoded_sentences) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #update the target sequence (of length 1):\n",
    "        output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        output_seq[0,0,sampled_token_index] = 1\n",
    "        \n",
    "        states_value = [h,c]\n",
    "    return decoded_sentences\n",
    "\n",
    "for seq_index in range(20):\n",
    "    # take one sequence for trying out decoding:\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentences = decode_sequence(input_seq)\n",
    "    print('ProjectGurukul Project: English to French Translation ')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:' , decoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_and_tokenize(sentence):\n",
    "#     sentence = sentence.replace('\\t', '').replace('\\n', '')\n",
    "#     tokens = word_tokenize(sentence.lower())\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081dd7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Nhat Thanh/nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m decode_sequence(input_seq)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Preprocess, tokenize, and convert to lowercase\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m decoded_tokens \u001b[38;5;241m=\u001b[39m preprocess_and_tokenize(decoded_sentence)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Print original and decoded sentences\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m, in \u001b[0;36mpreprocess_and_tokenize\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_and_tokenize\u001b[39m(sentence):\n\u001b[0;32m      3\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize and convert to lowercase\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Nhat Thanh/nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# meteor_references = []\n",
    "# meteor_hypotheses = []\n",
    "\n",
    "# for seq_index in range(20):\n",
    "#     input_seq = encoder_input_data[seq_index:seq_index + 1]\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "#     input_tokens = preprocess_and_tokenize(input_texts[seq_index])\n",
    "#     decoded_tokens = preprocess_and_tokenize(decoded_sentence)\n",
    "\n",
    "#     print('Input sentence:', ' '.join(input_tokens))\n",
    "#     print('Decoded sentence:', ' '.join(decoded_tokens))\n",
    "\n",
    "#     meteor_references.append(input_tokens)\n",
    "#     meteor_hypotheses.append(decoded_tokens)\n",
    "\n",
    "#     print('-' * 20)\n",
    "\n",
    "# meteor = meteor_score.corpus_meteor_score(meteor_references, meteor_hypotheses)\n",
    "# print('Overall METEOR Score:', meteor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
