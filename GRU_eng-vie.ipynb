{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "august-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU,Dense\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "breathing-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "latent_dim = 256\n",
    "num_samples = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "provincial-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'vie.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "piano-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "output_texts = []\n",
    "input_characters = set()\n",
    "output_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, output_text, _ = line.split('\\t')\n",
    "    \n",
    "    # We use 'tab' as the 'start sequence' character\n",
    "    # for the targets, and '\\n' as the 'end sequence' character.\n",
    "    \n",
    "    output_text = '\\t' + output_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in output_text:\n",
    "        if char not in output_characters:\n",
    "            output_characters.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attractive-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "earlier-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "output_characters = sorted(list(output_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(output_characters)\n",
    "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in output_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "greater-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 9428\n",
      "Number of unique input Tokens: 74\n",
      "Number of unique output Tokens: 164\n",
      "Max sequence length for inputs: 182\n",
      "Max sequence length for outputs: 163\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples:\", len(input_texts))\n",
    "print('Number of unique input Tokens:' , num_encoder_tokens)\n",
    "print('Number of unique output Tokens:' , num_decoder_tokens)\n",
    "print('Max sequence length for inputs:' , max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:' , max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hawaiian-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(input_characters)])\n",
    "\n",
    "output_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(output_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "comic-bunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({' ': 0,\n",
       "  '!': 1,\n",
       "  '\"': 2,\n",
       "  '$': 3,\n",
       "  '%': 4,\n",
       "  \"'\": 5,\n",
       "  '+': 6,\n",
       "  ',': 7,\n",
       "  '-': 8,\n",
       "  '.': 9,\n",
       "  '0': 10,\n",
       "  '1': 11,\n",
       "  '2': 12,\n",
       "  '3': 13,\n",
       "  '4': 14,\n",
       "  '5': 15,\n",
       "  '6': 16,\n",
       "  '7': 17,\n",
       "  '8': 18,\n",
       "  '9': 19,\n",
       "  ':': 20,\n",
       "  '?': 21,\n",
       "  'A': 22,\n",
       "  'B': 23,\n",
       "  'C': 24,\n",
       "  'D': 25,\n",
       "  'E': 26,\n",
       "  'F': 27,\n",
       "  'G': 28,\n",
       "  'H': 29,\n",
       "  'I': 30,\n",
       "  'J': 31,\n",
       "  'K': 32,\n",
       "  'L': 33,\n",
       "  'M': 34,\n",
       "  'N': 35,\n",
       "  'O': 36,\n",
       "  'P': 37,\n",
       "  'Q': 38,\n",
       "  'R': 39,\n",
       "  'S': 40,\n",
       "  'T': 41,\n",
       "  'U': 42,\n",
       "  'V': 43,\n",
       "  'W': 44,\n",
       "  'Y': 45,\n",
       "  'a': 46,\n",
       "  'b': 47,\n",
       "  'c': 48,\n",
       "  'd': 49,\n",
       "  'e': 50,\n",
       "  'f': 51,\n",
       "  'g': 52,\n",
       "  'h': 53,\n",
       "  'i': 54,\n",
       "  'j': 55,\n",
       "  'k': 56,\n",
       "  'l': 57,\n",
       "  'm': 58,\n",
       "  'n': 59,\n",
       "  'o': 60,\n",
       "  'p': 61,\n",
       "  'q': 62,\n",
       "  'r': 63,\n",
       "  's': 64,\n",
       "  't': 65,\n",
       "  'u': 66,\n",
       "  'v': 67,\n",
       "  'w': 68,\n",
       "  'x': 69,\n",
       "  'y': 70,\n",
       "  'z': 71,\n",
       "  '’': 72,\n",
       "  '€': 73},\n",
       " {'\\t': 0,\n",
       "  '\\n': 1,\n",
       "  ' ': 2,\n",
       "  '!': 3,\n",
       "  '\"': 4,\n",
       "  '$': 5,\n",
       "  '%': 6,\n",
       "  '(': 7,\n",
       "  ')': 8,\n",
       "  '+': 9,\n",
       "  ',': 10,\n",
       "  '-': 11,\n",
       "  '.': 12,\n",
       "  '/': 13,\n",
       "  '0': 14,\n",
       "  '1': 15,\n",
       "  '2': 16,\n",
       "  '3': 17,\n",
       "  '4': 18,\n",
       "  '5': 19,\n",
       "  '6': 20,\n",
       "  '7': 21,\n",
       "  '8': 22,\n",
       "  '9': 23,\n",
       "  ':': 24,\n",
       "  '>': 25,\n",
       "  '?': 26,\n",
       "  'A': 27,\n",
       "  'B': 28,\n",
       "  'C': 29,\n",
       "  'D': 30,\n",
       "  'E': 31,\n",
       "  'F': 32,\n",
       "  'G': 33,\n",
       "  'H': 34,\n",
       "  'I': 35,\n",
       "  'J': 36,\n",
       "  'K': 37,\n",
       "  'L': 38,\n",
       "  'M': 39,\n",
       "  'N': 40,\n",
       "  'O': 41,\n",
       "  'P': 42,\n",
       "  'Q': 43,\n",
       "  'R': 44,\n",
       "  'S': 45,\n",
       "  'T': 46,\n",
       "  'U': 47,\n",
       "  'V': 48,\n",
       "  'W': 49,\n",
       "  'X': 50,\n",
       "  'Y': 51,\n",
       "  'a': 52,\n",
       "  'b': 53,\n",
       "  'c': 54,\n",
       "  'd': 55,\n",
       "  'e': 56,\n",
       "  'f': 57,\n",
       "  'g': 58,\n",
       "  'h': 59,\n",
       "  'i': 60,\n",
       "  'j': 61,\n",
       "  'k': 62,\n",
       "  'l': 63,\n",
       "  'm': 64,\n",
       "  'n': 65,\n",
       "  'o': 66,\n",
       "  'p': 67,\n",
       "  'q': 68,\n",
       "  'r': 69,\n",
       "  's': 70,\n",
       "  't': 71,\n",
       "  'u': 72,\n",
       "  'v': 73,\n",
       "  'w': 74,\n",
       "  'x': 75,\n",
       "  'y': 76,\n",
       "  'z': 77,\n",
       "  'Á': 78,\n",
       "  'Â': 79,\n",
       "  'É': 80,\n",
       "  'Í': 81,\n",
       "  'Ô': 82,\n",
       "  'Ú': 83,\n",
       "  'Ý': 84,\n",
       "  'à': 85,\n",
       "  'á': 86,\n",
       "  'â': 87,\n",
       "  'ã': 88,\n",
       "  'è': 89,\n",
       "  'é': 90,\n",
       "  'ê': 91,\n",
       "  'ì': 92,\n",
       "  'í': 93,\n",
       "  'ò': 94,\n",
       "  'ó': 95,\n",
       "  'ô': 96,\n",
       "  'õ': 97,\n",
       "  'ù': 98,\n",
       "  'ú': 99,\n",
       "  'ý': 100,\n",
       "  'Ă': 101,\n",
       "  'ă': 102,\n",
       "  'Đ': 103,\n",
       "  'đ': 104,\n",
       "  'ĩ': 105,\n",
       "  'ũ': 106,\n",
       "  'ơ': 107,\n",
       "  'Ư': 108,\n",
       "  'ư': 109,\n",
       "  'ạ': 110,\n",
       "  'Ả': 111,\n",
       "  'ả': 112,\n",
       "  'Ấ': 113,\n",
       "  'ấ': 114,\n",
       "  'ầ': 115,\n",
       "  'ẩ': 116,\n",
       "  'ẫ': 117,\n",
       "  'ậ': 118,\n",
       "  'ắ': 119,\n",
       "  'ằ': 120,\n",
       "  'ẳ': 121,\n",
       "  'ẵ': 122,\n",
       "  'ặ': 123,\n",
       "  'ẹ': 124,\n",
       "  'ẻ': 125,\n",
       "  'ẽ': 126,\n",
       "  'ế': 127,\n",
       "  'ề': 128,\n",
       "  'ể': 129,\n",
       "  'ễ': 130,\n",
       "  'ệ': 131,\n",
       "  'ỉ': 132,\n",
       "  'ị': 133,\n",
       "  'ọ': 134,\n",
       "  'ỏ': 135,\n",
       "  'ố': 136,\n",
       "  'Ồ': 137,\n",
       "  'ồ': 138,\n",
       "  'Ổ': 139,\n",
       "  'ổ': 140,\n",
       "  'ỗ': 141,\n",
       "  'ộ': 142,\n",
       "  'ớ': 143,\n",
       "  'Ờ': 144,\n",
       "  'ờ': 145,\n",
       "  'Ở': 146,\n",
       "  'ở': 147,\n",
       "  'ỡ': 148,\n",
       "  'ợ': 149,\n",
       "  'ụ': 150,\n",
       "  'ủ': 151,\n",
       "  'Ứ': 152,\n",
       "  'ứ': 153,\n",
       "  'ừ': 154,\n",
       "  'ử': 155,\n",
       "  'ữ': 156,\n",
       "  'ự': 157,\n",
       "  'ỳ': 158,\n",
       "  'ỵ': 159,\n",
       "  'ỷ': 160,\n",
       "  'ỹ': 161,\n",
       "  '\\u200b': 162,\n",
       "  '€': 163})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index , output_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deluxe-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype = 'float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length , num_decoder_tokens), dtype = 'float32')\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens) , dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sought-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,output_text) in enumerate(zip(input_texts,output_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1.\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1.\n",
    "    \n",
    "    for t, char in enumerate(output_text):\n",
    "        # decoder_output_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i,t,output_token_index[char]] = 1.\n",
    "        if t>0:\n",
    "            # decoder_output_data will be ahead by one timestep\n",
    "            # and will not include the start character\n",
    "            decoder_output_data[i,t-1,output_token_index[char]] = 1.\n",
    "    decoder_input_data[i,t+1:, output_token_index[' ']] = 1.\n",
    "    decoder_output_data[i , t: , output_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sophisticated-singapore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 74)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "backed-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "encoder_outputs, state = encoder(encoder_inputs)\n",
    "encoder_states = [state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "outdoor-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, state = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "063c3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "involved-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "118/118 [==============================] - 248s 2s/step - loss: 0.9819 - accuracy: 0.8380 - f1_score: 0.8401 - val_loss: 1.3381 - val_accuracy: 0.7429 - val_f1_score: 0.7565\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 163s 1s/step - loss: 0.7060 - accuracy: 0.8459 - f1_score: 0.8793 - val_loss: 1.1286 - val_accuracy: 0.7466 - val_f1_score: 0.7918\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 114s 969ms/step - loss: 0.5941 - accuracy: 0.8589 - f1_score: 0.8926 - val_loss: 0.9412 - val_accuracy: 0.7726 - val_f1_score: 0.8024\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 110s 937ms/step - loss: 0.4891 - accuracy: 0.8770 - f1_score: 0.9052 - val_loss: 0.8243 - val_accuracy: 0.7912 - val_f1_score: 0.8303\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 97s 824ms/step - loss: 0.4404 - accuracy: 0.8846 - f1_score: 0.9128 - val_loss: 0.7712 - val_accuracy: 0.8018 - val_f1_score: 0.8380\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 95s 807ms/step - loss: 0.4231 - accuracy: 0.8895 - f1_score: 0.9154 - val_loss: 0.7365 - val_accuracy: 0.8077 - val_f1_score: 0.8398\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 94s 794ms/step - loss: 0.3939 - accuracy: 0.8937 - f1_score: 0.9183 - val_loss: 0.7061 - val_accuracy: 0.8138 - val_f1_score: 0.8412\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 97s 820ms/step - loss: 0.3788 - accuracy: 0.8970 - f1_score: 0.9201 - val_loss: 0.6906 - val_accuracy: 0.8157 - val_f1_score: 0.8453\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 95s 809ms/step - loss: 0.3674 - accuracy: 0.8995 - f1_score: 0.9216 - val_loss: 0.6719 - val_accuracy: 0.8188 - val_f1_score: 0.8482\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 100s 848ms/step - loss: 0.3584 - accuracy: 0.9021 - f1_score: 0.9232 - val_loss: 0.6553 - val_accuracy: 0.8239 - val_f1_score: 0.8501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22120103350>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# 'encoder_input_data' & 'decoder_input_data' into 'decoder_output_data'\n",
    "model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Run training:\n",
    "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy' , metrics = ['accuracy',f1_score])\n",
    "model.fit([encoder_input_data,decoder_input_data], decoder_output_data,\n",
    "         batch_size=batch_size,\n",
    "         epochs = epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pointed-counter",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"gru_3\" (type GRU).\n\nLayer has 1 states but was passed 2 initial states. Received: initial_state=[<tf.Tensor 'Placeholder_1:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(None, 256) dtype=float32>]\n\nCall arguments received by layer \"gru_3\" (type GRU):\n  • inputs=['tf.Tensor(shape=(None, None, 164), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m decoder_state_input_c \u001b[38;5;241m=\u001b[39m Input(shape \u001b[38;5;241m=\u001b[39m (latent_dim,))\n\u001b[0;32m      6\u001b[0m decoder_input_states \u001b[38;5;241m=\u001b[39m [decoder_state_input_h,decoder_state_input_c]\n\u001b[1;32m----> 8\u001b[0m decoder_outputs,state_h,state_c \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_gru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m decoder_states \u001b[38;5;241m=\u001b[39m [state_h,state_c]\n\u001b[0;32m     10\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m decoder_dense(decoder_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\base_rnn.py:615\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# Perform the call with temporarily replaced input_spec\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m full_input_spec\n\u001b[1;32m--> 615\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfull_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Remove the additional_specs from input spec and keep the rest. It\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# is important to keep since the input spec was populated by\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# build(), and will be reused in the stateful=True.\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(additional_specs)]\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\base_rnn.py:816\u001b[0m, in \u001b[0;36mRNN._process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    813\u001b[0m     initial_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_initial_state(inputs)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(initial_state) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates):\n\u001b[1;32m--> 816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates but was passed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(initial_state)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m initial \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates. Received: initial_state=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, initial_state, constants\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"gru_3\" (type GRU).\n\nLayer has 1 states but was passed 2 initial states. Received: initial_state=[<tf.Tensor 'Placeholder_1:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(None, 256) dtype=float32>]\n\nCall arguments received by layer \"gru_3\" (type GRU):\n  • inputs=['tf.Tensor(shape=(None, None, 164), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)', 'tf.Tensor(shape=(None, 256), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "# define sampling models:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape = (latent_dim,))\n",
    "decoder_state_input_c = Input(shape = (latent_dim,))\n",
    "decoder_input_states = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "decoder_outputs,state_h,state_c = decoder_gru(decoder_inputs, initial_state=decoder_input_states)\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs]+decoder_input_states,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i,char) for char,i in input_token_index.items())\n",
    "reverse_output_char_index = dict(\n",
    "    (i,char) for char,i in output_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value= encoder_model.predict(input_seq)\n",
    "    output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    output_seq[0,0,output_token_index['\\t']] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentences = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict(\n",
    "            [output_seq]+ states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1, :])\n",
    "        sampled_char = reverse_output_char_index[sampled_token_index]\n",
    "        decoded_sentences += sampled_char\n",
    "\n",
    "        if(sampled_char == '\\n' or len(decoded_sentences) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #update the target sequence (of length 1):\n",
    "        output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        output_seq[0,0,sampled_token_index] = 1\n",
    "        \n",
    "        states_value = [h,c]\n",
    "    return decoded_sentences\n",
    "\n",
    "for seq_index in range(20):\n",
    "    # take one sequence for trying out decoding:\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentences = decode_sequence(input_seq)\n",
    "    print('ProjectGurukul Project: English to French Translation ')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:' , decoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_and_tokenize(sentence):\n",
    "#     sentence = sentence.replace('\\t', '').replace('\\n', '')\n",
    "#     tokens = word_tokenize(sentence.lower())\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081dd7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Nhat Thanh/nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m decode_sequence(input_seq)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Preprocess, tokenize, and convert to lowercase\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m decoded_tokens \u001b[38;5;241m=\u001b[39m preprocess_and_tokenize(decoded_sentence)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Print original and decoded sentences\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m, in \u001b[0;36mpreprocess_and_tokenize\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_and_tokenize\u001b[39m(sentence):\n\u001b[0;32m      3\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize and convert to lowercase\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\Nhat Thanh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Nhat Thanh/nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Nhat Thanh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# meteor_references = []\n",
    "# meteor_hypotheses = []\n",
    "\n",
    "# for seq_index in range(20):\n",
    "#     input_seq = encoder_input_data[seq_index:seq_index + 1]\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "#     input_tokens = preprocess_and_tokenize(input_texts[seq_index])\n",
    "#     decoded_tokens = preprocess_and_tokenize(decoded_sentence)\n",
    "\n",
    "#     print('Input sentence:', ' '.join(input_tokens))\n",
    "#     print('Decoded sentence:', ' '.join(decoded_tokens))\n",
    "\n",
    "#     meteor_references.append(input_tokens)\n",
    "#     meteor_hypotheses.append(decoded_tokens)\n",
    "\n",
    "#     print('-' * 20)\n",
    "\n",
    "# meteor = meteor_score.corpus_meteor_score(meteor_references, meteor_hypotheses)\n",
    "# print('Overall METEOR Score:', meteor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
